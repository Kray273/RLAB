fore
par(mfrow=c(1,2))
plot(fore) # 향후 24개월 예측치 시각화
par(mfrow=c(1,1))
plot(fore) # 향후 24개월 예측치 시각화
model2 <- forecast(model, h = 6) # 향후 6개월 예측치 시각화
plot(model2)
# (1) 데이터 준비
data <- c(55,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,61,65,69,53,70,75,
56,56,65,55,68,80,65,67,77,69,79,82,
57,55,63,60,68,70,58,65,70,55,65,70)
length(data)# 48
# (2) 시계열자료 생성
tsdata <- ts(data, start=c(2016, 1),frequency=12)
#tsdata <- AirPassengers # 실제 data 적용.
tsdata
head(tsdata)
tail(tsdata)
# (3) 시계열요소분해 시각화
ts_feature <- stl(tsdata, s.window="periodic")
plot(ts_feature)
# 단계2 : 정상성시계열 변환
par(mfrow=c(1,2))
# 단계2 : 정상성시계열 변환
par(mfrow=c(1,1))
ts.plot(tsdata)
#사진2.
diff <- diff(tsdata)
#사진3.
plot(diff) # 차분 시각화
# 단계3 : 모형 식별과 추정
library(forecast)
ts_model2 <- auto.arima(tsdata)
ts_model2 # ARIMA(0,1,1)(1,1,0)[12] / ARIMA(2,1,1)(0,1,0)[12]
# 단계4 : 모형 생성
model <- arima(tsdata, c(0,1,1), seasonal = list(order = c(1,1,0)))
#model <- arima(tsdata, c(2,1,1), seasonal = list(order = c(0,1,0)))
model
# 단계5 : 모형 진단(모형 타당성 검정)
# (1) 자기상관함수에 의한 모형 진단
tsdiag(model)
# (2)Box-Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag=1, type = "Ljung") # p-value = 0.5618 / p-value = 0.9879
# 단계6 : 미래 예측
par(mfrow=c(1,2))
# 단계6 : 미래 예측
par(mfrow=c(1,1))
fore <- forecast(model, h=24) # 2년 예측
plot(fore)
fore2 <- forecast(model, h=6) # 6개월 예측
plot(fore2)
#사진1.
plot(SMA(tsdata, n=1), main="1년 단위 이동평균법으로 평활")
# 단계3 : 이동평균법으로 평활 및 시각화
par(mfrow=c(2,2))
plot(tsdata, main="원 시계열 자료") # 시계열 자료 시각화
#사진1.
plot(SMA(tsdata, n=1), main="1년 단위 이동평균법으로 평활")
#사진2.
plot(SMA(tsdata, n=2), main="2년 단위 이동평균법으로 평활")
#사진3.
plot(SMA(tsdata, n=3), main="3년 단위 이동평균법으로 평활")
par(mfrow=c(1,1))
# 단계2:정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
#사진2.
diff <- diff(tsdata)
plot(diff)
# 단계3: 모형 식별과 추정
install.packages('forecast')
install.packages("forecast")
arima <- auto.arima(tsdata) # 시계열 데이터 이용.
arima
arima <- auto.arima(tsdata) # 시계열 데이터 이용.
arima
# 단계1: 시계열자료 특성분석
# (1) 데이터 준비
input <- c(3180,3000,3200,3100,3300,3200,3400,3550,3200,3400,3300,3700)
# (2) 시계열 객체 생성(12개월:2015년 2월 ~ 2016년 1월)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
# (2) 시계열 객체 생성(12개월:2015년 2월 ~ 2016년 1월)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
tsdata
plot(tsdata, type="l", col='red')
#사진2.
diff <- diff(tsdata)
plot(diff)
# 단계2:정상성시계열 변환
par(mfrow=c(1,2))
ts.plot(tsdata)
#사진2.
diff <- diff(tsdata)
plot(diff)
arima <- auto.arima(tsdata) # 시계열 데이터 이용.
arima
# 단계4: 모형 생성
model <- arima(tsdata, order=c(1,1,0))
model
# 단계5: 모형 진단(모형 타당성 검정)
# (1) 자기상관함수에 의한 모형 진단
tsdiag(model)
# (2) Box-Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag = 1, type = "Ljung")
# 단계6 : 미래 예측(업무 적용)
fore <- forecast(model) # 향후 2년 예측
fore
par(mfrow=c(1,2))
plot(fore) # 향후 24개월 예측치 시각화
model2 <- forecast(model, h = 6) # 향후 6개월 예측치 시각화
plot(model2)
# (1) 데이터 준비
data <- c(55,56,45,43,69,75,58,59,66,64,62,65,
55,49,67,55,71,78,61,65,69,53,70,75,
56,56,65,55,68,80,65,67,77,69,79,82,
57,55,63,60,68,70,58,65,70,55,65,70)
length(data)# 48
# (2) 시계열자료 생성
tsdata <- ts(data, start=c(2016, 1),frequency=12)
#tsdata <- AirPassengers # 실제 data 적용.
tsdata
head(tsdata)
tail(tsdata)
# (3) 시계열요소분해 시각화
ts_feature <- stl(tsdata, s.window="periodic")
plot(ts_feature)
ts.plot(tsdata)
#사진2.
diff <- diff(tsdata)
plot(diff) # 차분 시각화
# (1) 패키지 설치 및 준비
getwd()
setwd("C:/workspaces/RLAB/data")
# (1) 패키지 설치 및 준비
getwd()
setwd("C:/workspaces/RLAB/data")
install.packages("rJava")
Sys.setenv(JAVA_HOME='C:/Program Files/Java/jdk-11.0.16.1')
library(rJava)
# 아래 패키지를 설치 후
install.packages("multilinguer")
library(multilinguer)
# 의존성을 설치 한다.
install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary")
library(stringr); library(hash); library(tau);
library(Sejong); library(RSQLite); library(devtools);
library(Sejong); library(RSQLite); library(devtools);
library(stringr); library(hash); library(tau);
library(Sejong); library(RSQLite); library(devtools);
# Git hub로 설치 한다. (remotes를 설치하기 위한 Git hub연동)
install.packages("remotes")
install.packages("remotes")
library(remotes)
remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"), force=TRUE)
library(KoNLP)
# KoNLP 테스트 예제
sentence <- '아버지가방에 스르륵 들어가신다.'
extractNoun(sentence)
# KoNLP 테스트 예제
sentence <- '아버지가 방에 스르륵 들어가신다.'
extractNoun(sentence) # 명사를 검색하는 함수로 형태소를 분석.
install.packages(c("wordcloud","tm"))
library(wordcloud); library(tm)
library(wordcloud); library(tm)
install.packages(c("wordcloud","tm")) # 형태소분석을 시각화 해주는 wordcloud와 택스트 마이닝을 실행하는 tm
install.packages(c("wordcloud", "tm"))
# (2) 텍스트 자료 가져오기
facebook <- file("facebook_bigdata.txt", encoding = "UTF-8") # 빅데이터라는 키워드로 SNS에서 긁어온것!
# 파일을 찾아가 파일형식은 UTF-8로 변경해주기!
facebook
# A connection with
# description "facebook_bigdata.txt"
# class       "file"
# mode        "r"
# text        "text"
# opened      "closed"
# can read    "yes"
# can write   "yes"
facebook_data <- readLines(facebook) # 줄 단위 데이터 생성
head(facebook_data) # 앞부분 6줄 보기 - 줄 단위 데이터 생성
# 사진 1. 라인단위로 구분을 지어서 번호가 매겨진다.
str(facebook_data) # chr [1:76]
close(facebook) #
# (3) 세종 사전에 신규 단어 추가
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# (3) 세종 사전에 신규 단어 추가
userDic <- data.frame(term=c("R 프로그래밍","페이스북","소셜네트워크","얼죽아"), tag='ncn')
# - 신규 단어 사전 추가 함수
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - 신규 단어 사전 추가 함수
buildDictionary(ext_dic = 'sejong', user_dic = userDic)
# - R 제공 함수로 단어 추출하기 - Sejong 사전에 등록된 신규 단어 테스트
paste(extractNoun("홍길동은 얼죽아를 최애로 생각하는, 빅데이터에 최대 관심을 가지고 있으면서, 페이스북이나 소셜네트워크로부터 생성 수집되어진 빅데이터 분석에 아주 많은 관심을 가지고 있어요."),collapse=" ")
# 단어 추출을 위한 사용자 정의 함수 정의하기
# (1) 사용자 정의 함수 작성
#     - [문자변환]->[명사 단어 추출]->[공백으로 합침]
exNouns <- function(x){
paste(extractNoun(as.character(x)), collapse = " ")
}
# (2) exNouns 함수 이용 단어 추출
facebook_nouns <- sapply(facebook_data, exNouns) # 명사 단어 추출.
# apply는 3개의 데이터를 받는다. 입력 매트리스 데이터-열,행 중심여부 - 함수, 입력으로 전달된 함수를 실행해서 보여준다.
# sapply는 입력된 데이터를 행렬로 반환한다. 백터
facebook_nouns[1] # 단어가 추출된 첫 줄 보기
# (5) 추출된 단어 대상 전처리
#  단계1: 추출된 단어 이용 말뭉치(Corpus) 생성
myCorpus <- Corpus(VectorSource(facebook_nouns)) # 택스트는 문장이여서 백터로 만들어준다.
myCorpus
#  단계2: 데이터 전처리
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변경
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, stopwords('english')) # 불용어 제거(for, very, and, of, are)
# 전처리 결과 확인
inspect(myCorpusPrepro[1:5])
myCorpusPrepro[1:5]
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, stopwords('english')) # 불용어 제거(for, very, and, of, are)
# 전처리 결과 확인
inspect(myCorpusPrepro[1:5])
myCorpusPrepro[1:5]
#  - Corpus 객체를 대상으로 TermDocumentMatrix() 함수를 이용하여 분석에 필요한 단어 선별하고 단어/문서 행렬을 만든다.
#  - 전처리된 단어집에서 단어 선별(단어 2음절 ~ 8음절 사이 단어)하기.
#  - 한글 1음절은 2byte에 저장(2음절=4byte)
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro, control=list(wordLengths=c(4,16)))
# 텍스트를 숫자로 표현하는 대표적인 방법. 객체를 생성!
# TermDocumentMatrix 엄밀히말하면 객체이다. 첫글자가 대문자인것으로 판단.
myCorpusPrepro_term # Corpus 객체 정보
# matrix 자료구조를 data.frame 자료 구조로 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
dim(myTerm_df) # [1] 696  76
#  2) 단어 선별-단어 길이 2~8개 이상 단어 선별.
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro, control=list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표적인 방법.
myCorpusPrepro_term # Corpus 객체 정보
# (7) 단어 출현 빈도수 구하기 - 빈도수가 높은 순서대로 내림차순 정렬.
wordResult <- sort(rowSums(myTerm_df), decreasing=T) # 빈도수로 내림차순 정렬.
wordResult[1:10]
# (8) 불필요한 단어 제거 시작
#   1) 데이터 전처리
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro,tolower) # 소문자 변경
myStopwords <- c(stopwords('english'), "사용", "하기")
myCorpusPrepro <- tm_map(myCorpusPrepro,removeWords, myStopwords) # 불용어 제거(for, very, and, of, are)
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인
#  2) 단어 선별-단어 길이 2~8개 이상 단어 선별.
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro, control=list(wordLengths=c(4,16))) # 텍스트를 숫자로 표현하는 대표적인 방법.
myCorpusPrepro_term # Corpus 객체 정보
# matrix 자료구조를 data.frame 자료 구조로 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
dim(myTerm_df) # [1] 696  76
#  3) 단어 출현 빈도수 구하기 - 빈도수가 높은 순서대로 내림차순 정렬.
wordResult <- sort(rowSums(myTerm_df), decreasing=T) # 빈도수로 내림차순 정렬.
wordResult[1:20]
myName <- names(wordResult) # 단어 이름 추출
wordcloud(myName, wordResult) # 단어 구름 시각화
# 단어 구름에 디자인 적용(빈도수, 색상, 위치, 회전등 )
# (1) 단어 이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
str(word.df)
# (2) 단어 색상과 글꼴 지정
pal <- brewer.pal(12, "Paired") # 12가지 색상 pal
windowsFonts(malgun=windowsFont("맑은 고딕"))
# (3) 단어 구름 시각화
wordcloud(word.df$word, word.df$freq, scale = c(5,1),
min.freq = 3, random.order = F, rot.per = .1,
colors = pal, family="malgun")
# 예시2) 텍스트 파일 가져오기와 단어 추출하기.
# 데이터 불러오기
txt <- readLines("hiphop.txt") # "UTF-8"로 변경.
# 멜론에서 hiphop으로 검색했을 때, 나온 노래의 가사를 저장.
head(txt)
# 특수문자 제거
txt <- str_replace_all(txt, "\\W", " ") # \W : 대문자 주의.
install.packages(stirngr)
install.packages("stirngr")
library(stirngr)
library(stringr); library(hash); library(tau);
# 특수문자 제거
txt <- str_replace_all(txt, "\\W", " ") # \W : 대문자 주의. 소문자 w는 특수문자를 검색색
head(txt)
# 가사에서 명사 추출
nouns <- extractNoun(txt)
nouns
# 추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성
wordcount <- table(unlist(nouns))
head(wordcount); tail(wordcount)
# 데이터프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
tail(df_word)
# 변수명 수정
names(df_word) <- c("word", "freq")
tail(df_word)
# 두 글자 이상 단어 추출
install.packages("dplyr")
library(dplyr)
df_word <- filter(df_word, nchar(word) >= 2)
top_20 <- df_word %>%
arrange(desc(freq)) %>%
head(20)
top_20
# 시각화
pal <- brewer.pal(8, "Dark2") # Dark2 색상 목록에서 8개  색상 추출.
# set.seed(1234)
wordcloud(words = df_word$word,
freq = df_word$freq,
min.freq = 2,
max.words = 200,
random.order = F,
rot.per = .1,
scale = c(4, 0.3),
colors = pal)
type(head(txt))
class(head(txt))
class(txt)
mode(txt)
?dplyr
?dplyr
# 텍스트 파일 가져오기와 단어 추출하기
# 1. 텍스트 파일 가져오기
marketing <- file("marketing.txt", encoding = "UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 데이터 생성
marketing2
close(marketing)
# 2. 줄 단위 단어 추출
lword <- Map(extractNoun, marketing2)
head(lword)
length(lword) # 472
View(lword)
lword <- unique(lword) # 공백 block 제거
length(lword) # 353
lword <- sapply(lword, unique)
length(lword) # 353
str(lword) # List of 353
lword <- unique(lword) # 공백 block 제거
length(lword) # 353
lword <- sapply(lword, unique)
length(lword) # 353
str(lword) # List of 353
# [1] " 보고 싶다"                   "이렇게 말하니까 더 보고 싶다" "너희 사진을 보고 있어도"      "보고 싶다"
# [5] "너무 야속한 시간"             "나는 우리가 밉다"
str(txt)
# 1) 단어 필터링 함수 정의 - 길이가 2개 이상 4개 이하 사이의 문자 길이로 구성된 단어만 필터링.
filter1 <- function(x){
nchar(x) >= 2 && nchar(x) <= 4 && is.hangul(x)
}
filter2 <- function(x){
Filter(filter1, x)
}
#  2) 줄 단위로 추출된 단어 전처리
lword <- sapply(lword, filter2) # 단어 길이가 1이하 또는 5 이상인 단어 제거.
lword
# 1) 연관 분석을 위한 패키지 설치
install.packages("arules")
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions")
wordtran
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions")
# 1) 연관 분석을 위한 패키지 설치
install.packages("arules")
library(arules)
# 2) 트랜잭션 생성
wordtran <- as(lword, "transactions")
wordtran
# 3) 교차표 작성: crossTable() -> 교차테이블 함수를 이용.
wordtable <- crossTable(wordtran)
wordtable
# 4) 단어 간 연관 규칙 산출
tranrules <- apriori(wordtran,parameter=list(support=0.25, conf=0.05))
# 5) 연관 규칙 생성 결과 보기
inspect(tranrules)
# 연관어 시각화
# 1) 연관 단어 시각화를 위해서 자료 구조 변경
rules <- labels(tranrules, ruleSep=" ") # 연관규칙 레이블을 " "으로 분리
head(rules, 20)
class(rules) # "character"
# 2) 문자열로 묶인 연관 단어를 행렬 구조 변경.
rules <- sapply(rules, strsplit, " ", USE.NAMES = F)
rules
class(rules) # "list"
# 3) 행 단위로 묶어서 matrix로 반환
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat) # "matrix"
# 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph")
library(igraph)
# edgelist 보기 - 연관 단어를 정점(Vertex) 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed = F) #[1,]~[11,] "{}" 제외
ruleg
plot.igraph(ruleg,vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green',
vertex.frame.color='blue')
#  (1) 패키지 설치 및 준비
# 실습: 웹 문서 요청과 파싱 관련 패키지 설치 및 로딩
install.packages("httr")
library(httr)
install.packages("XML")
library(XML)
# 실습: 웹 문서 요청
url <- "https://news.daum.net"
web <- GET(url)
web
# 실습: HTML 파싱하기
html <- htmlTreeParse(web, useInternalNodes = T, trim = T, encoding = "utf-8")
rootNode <- xmlRoot(html)
html
# 실습: 태그 자료 수집하기
news <- xpathSApply(rootNode, "//a[@class = 'link_txt']", xmlValue)
news
web
html
news
# 실습: 자료 전처리하기
# 단계 1: 자료 전처리 - 수집한 문서를 대상으로 불용어 제거
news_pre <- gsub("[\r\n\t]", ' ', news)
news_pre <- gsub('[[:punct:]]', ' ', news_pre)
news_pre <- gsub('[[:cntrl:]]', ' ', news_pre)
# news_pre <- gsub('\\d+', ' ', news_pre)   # corona19(covid19) 때문에 숫자 제거 생략
news_pre <- gsub('[a-z]+', ' ', news_pre)
news_pre <- gsub('[A-Z]+', ' ', news_pre)
news_pre <- gsub('\\s+', ' ', news_pre)
news_pre
# 단계 2: 기사와 관계 없는 'TODAY', '검색어 순위' 등의 내용은 제거
news_data <- news_pre[1:32] # 검색수 만큼 변경
news_data
news_pre <- gsub('\\d+', ' ', news_pre)   # corona19(covid19) 때문에 숫자 제거 생략, 일반적으로 제거
news_data # 사진2.
news_pre <- gsub('\\d+', ' ', news_pre)   # corona19(covid19) 때문에 숫자 제거 생략, 일반적으로 제거
news_pre #사진 1.
# 단계 2: 기사와 관계 없는 'TODAY', '검색어 순위' 등의 내용은 제거
news_data <- news_pre[1:32] # 검색수 만큼 변경
news_data # 사진2.
news_data # 사진2.
# 실습: 수집한 자료를 파일로 저장하고 읽기
write.csv(news_data, "C:/workspaces/RLAB/outputs/news_data.csv", quote = F)
news_data <- read.csv("C:/workspaces/RLAB/outputs/news_data.csv", header = T, stringsAsFactors = F)
str(news_data)
names(news_data) <- c("no", "news_text")
head(news_data)
news_text <- news_data$news_text
news_text
# 실습: 세종 사전에 단어 추가
user_dic <- data.frame(term = c("이태원역", "탄도미사일", "이태원"), tag = 'ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
# 실습: 세종 사전에 단어 추가
user_dic <- data.frame(term = c("이태원역", "탄도미사일", "이태원"), tag = 'ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
# 실습: 세종 사전에 단어 추가
library(KoNLP)
user_dic <- data.frame(term = c("이태원역", "탄도미사일", "이태원"), tag = 'ncn')
buildDictionary(ext_dic = 'sejong', user_dic = user_dic)
# 실습: 단어 추출 사용자 함수 정의하기
# 단계 1: 사용자 정의 함수 작성
exNouns <- function(x) { paste(extractNoun(x), collapse = " ")}
# 단계 2: exNouns()  함수를 이용하어 단어 추출
news_nouns <- sapply(news_text, exNouns)
news_nouns
# 단계 3: 추출 결과 확인
str(news_nouns)
# 실습: 말뭉치 생성과 집계 행렬 만들기
# 단계 1: 추출된 단어를 이용한 말뭉치(corpus) 생성
newsCorpus <- Corpus(VectorSource(news_nouns))
# 실습: 말뭉치 생성과 집계 행렬 만들기
# 단계 1: 추출된 단어를 이용한 말뭉치(corpus) 생성
library(Corpus)
# 실습: 말뭉치 생성과 집계 행렬 만들기
# 단계 1: 추출된 단어를 이용한 말뭉치(corpus) 생성
install.packages('Corpus')
library(Corpus)
# 실습: 말뭉치 생성과 집계 행렬 만들기
# 단계 1: 추출된 단어를 이용한 말뭉치(corpus) 생성
install.packages('tm')
library(tm)
newsCorpus <- Corpus(VectorSource(news_nouns))
newsCorpus
#<<SimpleCorpus>>
#Metadata:  corpus specific: 1, document level (indexed): 0
#Content:  documents: 32
inspect(newsCorpus)
# 단계 2: 단어 vs 문서 집계 행렬 만들기
# 한글 2~8 음절 단어 대상 단어/문서 집계 행렬
TDM <- TermDocumentMatrix(newsCorpus, control = list(wordLengths = c(4, 16)))
TDM
TDM
# 단계 3: matrix 자료구조를 data.frame 자료구조로 변경
tdm.df <- as.data.frame(as.matrix(TDM))
dim(tdm.df)
# 실습: 단어 출현 빈도수 구하기
wordResult <- sort(rowSums(tdm.df), decreasing = TRUE)
wordResult[1:10]
# 실습: 단어 구름 생성
# 단계 1: 패키지 로딩과 단어 이름 추출
library(wordcloud)
myNames <- names(wordResult)
myNames
# 단계 2: 단어와 단어 빈도수 구하기
df <- data.frame(word = myNames, freq = wordResult)
head(df)
# 단계 3: 단어 구름 생성
pal <- brewer.pal(12, "Paired")
wordcloud(df$word, df$freq, min.freq = 2,
random.order = F, scale = c(4, 0.7),
rot.per = .1, colors = pal)
wordcloud(df$word, df$freq, min.freq = 1,
random.order = F, scale = c(4, 0.7),
rot.per = .1, colors = pal)
myNames #사진1.
